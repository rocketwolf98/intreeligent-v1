{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "309604f5-dc65-4624-ac29-77fb9682a4ea",
   "metadata": {},
   "source": [
    "# Experimental Orchestration Pipeline\n",
    "## (With Tiling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "39a7cddb-6c3e-48f7-bed8-dc8f34202136",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-06T00:48:33.693636Z",
     "iopub.status.busy": "2025-10-06T00:48:33.693357Z",
     "iopub.status.idle": "2025-10-06T00:48:36.287236Z",
     "shell.execute_reply": "2025-10-06T00:48:36.286585Z",
     "shell.execute_reply.started": "2025-10-06T00:48:33.693608Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /opt/conda/lib/python3.12/site-packages (2.6.0)\n",
      "Requirement already satisfied: torchvision in /opt/conda/lib/python3.12/site-packages (0.21.0)\n",
      "Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.12/site-packages (1.7.1)\n",
      "Requirement already satisfied: hdbscan in /opt/conda/lib/python3.12/site-packages (0.8.40)\n",
      "Requirement already satisfied: plotly in /opt/conda/lib/python3.12/site-packages (6.0.1)\n",
      "Requirement already satisfied: umap-learn in /opt/conda/lib/python3.12/site-packages (0.5.9.post2)\n",
      "Requirement already satisfied: opencv-python in /opt/conda/lib/python3.12/site-packages (4.12.0.88)\n",
      "Collecting rasterio\n",
      "  Downloading rasterio-1.4.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.1 kB)\n",
      "Requirement already satisfied: pycocotools in /opt/conda/lib/python3.12/site-packages (2.0.10)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.12/site-packages (from torch) (3.19.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /opt/conda/lib/python3.12/site-packages (from torch) (4.14.1)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.12/site-packages (from torch) (80.9.0)\n",
      "Requirement already satisfied: sympy!=1.13.2,>=1.13.1 in /opt/conda/lib/python3.12/site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.12/site-packages (from torch) (3.5)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.12/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.12/site-packages (from torch) (2024.12.0)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.12/site-packages (from torchvision) (2.2.6)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.12/site-packages (from torchvision) (11.3.0)\n",
      "Requirement already satisfied: scipy>=1.8.0 in /opt/conda/lib/python3.12/site-packages (from scikit-learn) (1.16.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /opt/conda/lib/python3.12/site-packages (from scikit-learn) (1.5.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/conda/lib/python3.12/site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: narwhals>=1.15.1 in /opt/conda/lib/python3.12/site-packages (from plotly) (2.1.2)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.12/site-packages (from plotly) (24.2)\n",
      "Requirement already satisfied: numba>=0.51.2 in /opt/conda/lib/python3.12/site-packages (from umap-learn) (0.61.2)\n",
      "Requirement already satisfied: pynndescent>=0.5 in /opt/conda/lib/python3.12/site-packages (from umap-learn) (0.5.13)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.12/site-packages (from umap-learn) (4.67.1)\n",
      "Collecting affine (from rasterio)\n",
      "  Downloading affine-2.4.0-py3-none-any.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: attrs in /opt/conda/lib/python3.12/site-packages (from rasterio) (23.2.0)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.12/site-packages (from rasterio) (2025.8.3)\n",
      "Requirement already satisfied: click>=4.0 in /opt/conda/lib/python3.12/site-packages (from rasterio) (8.2.1)\n",
      "Collecting cligj>=0.5 (from rasterio)\n",
      "  Downloading cligj-0.7.2-py3-none-any.whl.metadata (5.0 kB)\n",
      "Collecting click-plugins (from rasterio)\n",
      "  Downloading click_plugins-1.1.1.2-py2.py3-none-any.whl.metadata (6.5 kB)\n",
      "Requirement already satisfied: pyparsing in /opt/conda/lib/python3.12/site-packages (from rasterio) (3.2.3)\n",
      "Requirement already satisfied: llvmlite<0.45,>=0.44.0dev0 in /opt/conda/lib/python3.12/site-packages (from numba>=0.51.2->umap-learn) (0.44.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.12/site-packages (from sympy!=1.13.2,>=1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.12/site-packages (from jinja2->torch) (3.0.2)\n",
      "Downloading rasterio-1.4.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (22.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m22.3/22.3 MB\u001b[0m \u001b[31m154.6 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading cligj-0.7.2-py3-none-any.whl (7.1 kB)\n",
      "Downloading affine-2.4.0-py3-none-any.whl (15 kB)\n",
      "Downloading click_plugins-1.1.1.2-py2.py3-none-any.whl (11 kB)\n",
      "Installing collected packages: cligj, click-plugins, affine, rasterio\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4/4\u001b[0m [rasterio]3/4\u001b[0m [rasterio]\n",
      "\u001b[1A\u001b[2KSuccessfully installed affine-2.4.0 click-plugins-1.1.1.2 cligj-0.7.2 rasterio-1.4.3\n"
     ]
    }
   ],
   "source": [
    "!pip install torch torchvision scikit-learn hdbscan plotly umap-learn opencv-python rasterio pycocotools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8bca25da-f1b6-40ad-872e-ab6fcdade434",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-06T00:49:43.581772Z",
     "iopub.status.busy": "2025-10-06T00:49:43.581483Z",
     "iopub.status.idle": "2025-10-06T00:49:43.642089Z",
     "shell.execute_reply": "2025-10-06T00:49:43.641506Z",
     "shell.execute_reply.started": "2025-10-06T00:49:43.581750Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import pytorch_lightning as pl\n",
    "import numpy as np\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple, Optional, Union\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score\n",
    "\n",
    "from dataclasses import dataclass, field\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.cluster import DBSCAN\n",
    "import hdbscan\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.express as px\n",
    "from tqdm.notebook import tqdm\n",
    "from pycocotools import mask as mask_util\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Try importing rasterio for GeoTIFF support\n",
    "try:\n",
    "    import rasterio\n",
    "    from rasterio.windows import Window\n",
    "    RASTERIO_AVAILABLE = True\n",
    "except ImportError:\n",
    "    RASTERIO_AVAILABLE = False\n",
    "    print(\"⚠ rasterio not available. GeoTIFF support limited. Install with: pip install rasterio\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73a3a514-e4c4-44f3-864c-a4185149e5bb",
   "metadata": {},
   "source": [
    "<h2>Defining Functions </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8cca5fc-95ef-4918-9066-5f23be459335",
   "metadata": {},
   "source": [
    "<h4>Configuration</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3f4080e8-1862-42fb-8c21-439fb0057f12",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-06T00:54:33.271158Z",
     "iopub.status.busy": "2025-10-06T00:54:33.270700Z",
     "iopub.status.idle": "2025-10-06T00:54:33.315778Z",
     "shell.execute_reply": "2025-10-06T00:54:33.315227Z",
     "shell.execute_reply.started": "2025-10-06T00:54:33.271133Z"
    }
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class OrthomosaicConfig:\n",
    "    \"\"\"Configuration for orthomosaic processing pipeline\"\"\"\n",
    "    # Paths\n",
    "    orthomosaic_path: str\n",
    "    maskrcnn_checkpoint: str\n",
    "    autoencoder_checkpoint: str\n",
    "    output_dir: str = \"outputs/orthomosaic\"\n",
    "    \n",
    "    # Tiling parameters\n",
    "    tile_size: int = 1024\n",
    "    tile_overlap_percent: int = 20  # 10-25%\n",
    "    \n",
    "    # Image sizes for models\n",
    "    maskrcnn_size: int = 500\n",
    "    autoencoder_size: int = 512\n",
    "    \n",
    "    # Detection thresholds\n",
    "    mask_threshold: float = 0.5\n",
    "    score_threshold: float = 0.5\n",
    "    \n",
    "    # Clustering\n",
    "    clustering_method: str = \"HDBSCAN\"  # \"DBSCAN\" or \"HDBSCAN\"\n",
    "    subset_ratio: float = 0.25\n",
    "    \n",
    "    # Duplicate merging\n",
    "    iou_threshold: float = 0.5\n",
    "    \n",
    "    # Visualization\n",
    "    preview_max_size: int = 2048  # Downsample to this for preview\n",
    "    enable_tile_previews: bool = True  # Show tiles as they're processed\n",
    "    \n",
    "    # Processing\n",
    "    batch_tiles: int = 1  # Process N tiles at once (increase if more RAM available)\n",
    "    save_intermediate: bool = True  # Save per-tile results\n",
    "    \n",
    "    # Device\n",
    "    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bb654bd-f99d-4704-a9b9-129730fe47ba",
   "metadata": {},
   "source": [
    "<h4>Model Loading</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a21a49-b946-4f86-8fac-c6dc9e3925d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PredictionProcessor:\n",
    "    \"\"\"Process Mask R-CNN predictions\"\"\"\n",
    "    def __init__(self, mask_threshold=0.5, score_threshold=0.5):\n",
    "        self.mask_threshold = mask_threshold\n",
    "        self.score_threshold = score_threshold\n",
    "    \n",
    "    def __call__(self, predictions):\n",
    "        \"\"\"Filter predictions by score threshold\"\"\"\n",
    "        if isinstance(predictions, list):\n",
    "            predictions = predictions[0]\n",
    "        \n",
    "        # Filter by score\n",
    "        keep = predictions['scores'] > self.score_threshold\n",
    "        \n",
    "        filtered = {\n",
    "            'boxes': predictions['boxes'][keep],\n",
    "            'masks': predictions['masks'][keep],\n",
    "            'scores': predictions['scores'][keep],\n",
    "            'labels': predictions['labels'][keep]\n",
    "        }\n",
    "        \n",
    "        # Threshold masks\n",
    "        filtered['masks'] = (filtered['masks'] > self.mask_threshold).squeeze(1)\n",
    "        \n",
    "        return filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae999a0-7e4d-498a-ba60-c38b64069215",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LitMaskRCNN(pl.LightningModule):\n",
    "    \"\"\"PyTorch Lightning Mask R-CNN Module\"\"\"\n",
    "    def __init__(self, lr=1e-3, num_classes=2, mask_threshold=0.5, score_threshold=0.5):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        \n",
    "        # Load pretrained Mask R-CNN with ResNet50 FPN backbone\n",
    "        self.model = torchvision.models.detection.maskrcnn_resnet50_fpn(weights=\"DEFAULT\")\n",
    "        \n",
    "        # Replace box predictor head\n",
    "        in_features = self.model.roi_heads.box_predictor.cls_score.in_features\n",
    "        self.model.roi_heads.box_predictor = \\\n",
    "            torchvision.models.detection.faster_rcnn.FastRCNNPredictor(in_features, num_classes)\n",
    "        \n",
    "        # Replace mask predictor head\n",
    "        in_features_mask = self.model.roi_heads.mask_predictor.conv5_mask.in_channels\n",
    "        hidden_layer = 256\n",
    "        self.model.roi_heads.mask_predictor = \\\n",
    "            torchvision.models.detection.mask_rcnn.MaskRCNNPredictor(\n",
    "                in_features_mask, hidden_layer, num_classes\n",
    "            )\n",
    "        \n",
    "        # Prediction post-processor\n",
    "        self.pred_processor = PredictionProcessor(\n",
    "            mask_threshold=mask_threshold,\n",
    "            score_threshold=score_threshold\n",
    "        )\n",
    "    \n",
    "    def forward(self, images, targets=None):\n",
    "        return self.model(images, targets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89010e71-d7e7-4ef6-9375-1b5ba2e76c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_maskrcnn(checkpoint_path: str, config: PipelineConfig) -> LitMaskRCNN:\n",
    "    \"\"\"Load Mask R-CNN model from checkpoint\"\"\"\n",
    "    print(f\"Loading Mask R-CNN from {checkpoint_path}...\")\n",
    "    model = LitMaskRCNN.load_from_checkpoint(\n",
    "        checkpoint_path,\n",
    "        mask_threshold=config.mask_threshold,\n",
    "        score_threshold=config.score_threshold\n",
    "    )\n",
    "    model.eval()\n",
    "    model.to(config.device)\n",
    "    print(\"✓ Mask R-CNN loaded successfully\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c15cf27-3e98-44c6-a830-fb1c9869f91e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_autoencoder(checkpoint_path: str, config: PipelineConfig, model_class):\n",
    "    \"\"\"Load autoencoder model from .pth file\"\"\"\n",
    "    print(f\"Loading Autoencoder from {checkpoint_path}...\")\n",
    "    model = model_class  # Initialize your TreeCrownResNet34 or similar\n",
    "    model.load_state_dict(torch.load(checkpoint_path, map_location=config.device))\n",
    "    model.eval()\n",
    "    model.to(config.device)\n",
    "    print(\"✓ Autoencoder loaded successfully\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52eba41f-0701-46ab-ae0d-1f71516e0f7b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "53776890-3747-49f8-82ac-2619430da667",
   "metadata": {},
   "source": [
    "<h4>Image Processing and Detection</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a83a6259-ae3a-440a-800d-f28941a38e98",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-05T12:36:40.030568Z",
     "iopub.status.busy": "2025-10-05T12:36:40.030352Z",
     "iopub.status.idle": "2025-10-05T12:36:40.034097Z",
     "shell.execute_reply": "2025-10-05T12:36:40.033529Z",
     "shell.execute_reply.started": "2025-10-05T12:36:40.030550Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_and_preprocess_image(image_path: str, target_size: int) -> Tuple[torch.Tensor, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Load image and prepare for Mask R-CNN inference\n",
    "    \n",
    "    Returns:\n",
    "        torch_image: (C, H, W) tensor for model input\n",
    "        original_np: (H, W, C) numpy array for visualization\n",
    "    \"\"\"\n",
    "    # Load image\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    original_np = np.array(image)\n",
    "    \n",
    "    # Resize to target size\n",
    "    image_resized = image.resize((target_size, target_size), Image.BILINEAR)\n",
    "    \n",
    "    # Convert to tensor and normalize\n",
    "    image_tensor = torchvision.transforms.functional.to_tensor(image_resized)\n",
    "    \n",
    "    return image_tensor, original_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3568e460-4230-4434-8db3-21526d54ee50",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-05T12:36:40.474605Z",
     "iopub.status.busy": "2025-10-05T12:36:40.474381Z",
     "iopub.status.idle": "2025-10-05T12:36:40.478366Z",
     "shell.execute_reply": "2025-10-05T12:36:40.477844Z",
     "shell.execute_reply.started": "2025-10-05T12:36:40.474587Z"
    }
   },
   "outputs": [],
   "source": [
    "def run_detection(model: LitMaskRCNN, image_tensor: torch.Tensor, config: PipelineConfig) -> Dict:\n",
    "    \"\"\"Run Mask R-CNN detection on image\"\"\"\n",
    "    print(\"Running Mask R-CNN detection...\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        image_batch = image_tensor.unsqueeze(0).to(config.device)\n",
    "        predictions = model(image_batch)\n",
    "        predictions_processed = model.pred_processor(predictions)\n",
    "    \n",
    "    # Move to CPU\n",
    "    for key in predictions_processed:\n",
    "        predictions_processed[key] = predictions_processed[key].cpu()\n",
    "    \n",
    "    num_detections = len(predictions_processed['scores'])\n",
    "    print(f\"✓ Detected {num_detections} tree crowns\")\n",
    "    \n",
    "    return predictions_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "29da35f5-66ce-4d2b-9fff-a44ee8f79852",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-05T12:36:40.809577Z",
     "iopub.status.busy": "2025-10-05T12:36:40.809375Z",
     "iopub.status.idle": "2025-10-05T12:36:40.813182Z",
     "shell.execute_reply": "2025-10-05T12:36:40.812661Z",
     "shell.execute_reply.started": "2025-10-05T12:36:40.809560Z"
    }
   },
   "outputs": [],
   "source": [
    "def scale_predictions(predictions: Dict, from_size: int, to_size: int) -> Dict:\n",
    "    \"\"\"Scale prediction coordinates from one size to another\"\"\"\n",
    "    scale_factor = to_size / from_size\n",
    "    \n",
    "    scaled_preds = predictions.copy()\n",
    "    scaled_preds['boxes'] = predictions['boxes'] * scale_factor\n",
    "    \n",
    "    # Scale masks\n",
    "    masks = predictions['masks'].numpy()\n",
    "    scaled_masks = []\n",
    "    for mask in masks:\n",
    "        scaled_mask = cv2.resize(\n",
    "            mask.astype(np.uint8),\n",
    "            (to_size, to_size),\n",
    "            interpolation=cv2.INTER_NEAREST\n",
    "        )\n",
    "        scaled_masks.append(scaled_mask)\n",
    "    \n",
    "    scaled_preds['masks'] = torch.from_numpy(np.array(scaled_masks))\n",
    "    \n",
    "    return scaled_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c4a7a8-402d-47a1-a176-005c51059e37",
   "metadata": {},
   "source": [
    "<h4>Mask Cropping and Feature Extraction</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "50d9180e-c450-4406-954d-a80d3970ad57",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-05T12:36:41.600162Z",
     "iopub.status.busy": "2025-10-05T12:36:41.599930Z",
     "iopub.status.idle": "2025-10-05T12:36:41.604948Z",
     "shell.execute_reply": "2025-10-05T12:36:41.604374Z",
     "shell.execute_reply.started": "2025-10-05T12:36:41.600144Z"
    }
   },
   "outputs": [],
   "source": [
    "def crop_masks(image_np: np.ndarray, predictions: Dict, config: PipelineConfig) -> List[np.ndarray]:\n",
    "    \"\"\"\n",
    "    Crop individual tree crown masks with transparent background\n",
    "    \n",
    "    Returns:\n",
    "        List of RGBA images (H, W, 4) with transparent backgrounds\n",
    "    \"\"\"\n",
    "    print(\"Cropping individual tree crown masks...\")\n",
    "    crops = []\n",
    "    \n",
    "    boxes = predictions['boxes'].numpy()\n",
    "    masks = predictions['masks'].numpy()\n",
    "    \n",
    "    for i, (box, mask) in enumerate(zip(boxes, masks)):\n",
    "        x1, y1, x2, y2 = box.astype(int)\n",
    "        \n",
    "        # Clip to image bounds\n",
    "        x1, y1 = max(0, x1), max(0, y1)\n",
    "        x2 = min(image_np.shape[1], x2)\n",
    "        y2 = min(image_np.shape[0], y2)\n",
    "        \n",
    "        # Crop image and mask\n",
    "        crop_rgb = image_np[y1:y2, x1:x2]\n",
    "        crop_mask = mask[y1:y2, x1:x2]\n",
    "        \n",
    "        # Create RGBA with transparent background\n",
    "        crop_rgba = np.zeros((*crop_rgb.shape[:2], 4), dtype=np.uint8)\n",
    "        crop_rgba[:, :, :3] = crop_rgb\n",
    "        crop_rgba[:, :, 3] = (crop_mask * 255).astype(np.uint8)\n",
    "        \n",
    "        crops.append(crop_rgba)\n",
    "    \n",
    "    print(f\"✓ Created {len(crops)} mask crops\")\n",
    "    return crops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b77a350a-46e6-4ef9-9c49-60b14b82b5b0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-05T12:36:41.999175Z",
     "iopub.status.busy": "2025-10-05T12:36:41.998982Z",
     "iopub.status.idle": "2025-10-05T12:36:42.002222Z",
     "shell.execute_reply": "2025-10-05T12:36:42.001702Z",
     "shell.execute_reply.started": "2025-10-05T12:36:41.999157Z"
    }
   },
   "outputs": [],
   "source": [
    "def resize_crops(crops: List[np.ndarray], target_size: int) -> List[np.ndarray]:\n",
    "    \"\"\"Resize crops to target size for autoencoder\"\"\"\n",
    "    resized = []\n",
    "    for crop in crops:\n",
    "        resized_crop = cv2.resize(crop, (target_size, target_size), interpolation=cv2.INTER_LINEAR)\n",
    "        resized.append(resized_crop)\n",
    "    return resized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bf070a46-18bc-42c0-b401-49dd6ae94fb2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-05T12:36:42.434651Z",
     "iopub.status.busy": "2025-10-05T12:36:42.434461Z",
     "iopub.status.idle": "2025-10-05T12:36:42.438921Z",
     "shell.execute_reply": "2025-10-05T12:36:42.438394Z",
     "shell.execute_reply.started": "2025-10-05T12:36:42.434634Z"
    }
   },
   "outputs": [],
   "source": [
    "def extract_features(autoencoder, crops: List[np.ndarray], config: PipelineConfig) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Extract latent features from crops using autoencoder\n",
    "    \n",
    "    Returns:\n",
    "        features: (N, latent_dim) numpy array\n",
    "    \"\"\"\n",
    "    print(\"Extracting latent features from autoencoder...\")\n",
    "    \n",
    "    features_list = []\n",
    "    \n",
    "    # Process in batches\n",
    "    batch_size = 16\n",
    "    for i in range(0, len(crops), batch_size):\n",
    "        batch_crops = crops[i:i + batch_size]\n",
    "        \n",
    "        # Convert to tensor (take RGB channels only)\n",
    "        batch_tensors = []\n",
    "        for crop in batch_crops:\n",
    "            # Convert RGBA to RGB and normalize\n",
    "            rgb = crop[:, :, :3].astype(np.float32) / 255.0\n",
    "            tensor = torch.from_numpy(rgb).permute(2, 0, 1)  # (C, H, W)\n",
    "            batch_tensors.append(tensor)\n",
    "        \n",
    "        batch = torch.stack(batch_tensors).to(config.device)\n",
    "        \n",
    "        # Extract features\n",
    "        with torch.no_grad():\n",
    "            latent = autoencoder.encode(batch)\n",
    "            features_list.append(latent.cpu().numpy())\n",
    "    \n",
    "    features = np.vstack(features_list)\n",
    "    print(f\"✓ Extracted features with shape {features.shape}\")\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "112e4b99-d5a4-468d-a837-452b67194f91",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-05T12:36:44.094133Z",
     "iopub.status.busy": "2025-10-05T12:36:44.093875Z",
     "iopub.status.idle": "2025-10-05T12:36:44.102629Z",
     "shell.execute_reply": "2025-10-05T12:36:44.102070Z",
     "shell.execute_reply.started": "2025-10-05T12:36:44.094113Z"
    }
   },
   "outputs": [],
   "source": [
    "def optimize_clustering_parameters(\n",
    "    features: np.ndarray,\n",
    "    config: PipelineConfig,\n",
    "    param_grid: Optional[Dict] = None\n",
    ") -> Tuple[Dict, float]:\n",
    "    \"\"\"\n",
    "    Optimize clustering parameters using grid search on subset\n",
    "    \n",
    "    Returns:\n",
    "        best_params: Dictionary of best parameters\n",
    "        best_score: Best silhouette score achieved\n",
    "    \"\"\"\n",
    "    print(\"\\nOptimizing clustering parameters...\")\n",
    "    \n",
    "    # Sample subset\n",
    "    n_samples = len(features)\n",
    "    subset = features\n",
    "    \n",
    "    print(f\"Using {n_samples} samples for optimization\")\n",
    "    \n",
    "    # Default parameter grids\n",
    "    if param_grid is None:\n",
    "        if config.clustering_method == \"DBSCAN\":\n",
    "            param_grid = {\n",
    "                'eps': [0.5, 1.0, 1.5, 2.0, 2.5, 3.0],\n",
    "                'min_samples': [3, 5, 10, 15, 20]\n",
    "            }\n",
    "        else:  # HDBSCAN\n",
    "            param_grid = {\n",
    "                'min_cluster_size': [10, 15, 20, 30, 50],\n",
    "                'min_samples': [3, 5, 10, 15]\n",
    "            }\n",
    "    \n",
    "    best_score = -1\n",
    "    best_params = None\n",
    "    results = []\n",
    "    \n",
    "    # Grid search\n",
    "    if config.clustering_method == \"DBSCAN\":\n",
    "        for eps in param_grid['eps']:\n",
    "            for min_samples in param_grid['min_samples']:\n",
    "                clusterer = DBSCAN(eps=eps, min_samples=min_samples, metric='euclidean')\n",
    "                labels = clusterer.fit_predict(subset)\n",
    "                \n",
    "                # Calculate metrics (only if we have more than 1 cluster)\n",
    "                n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "                if n_clusters > 1:\n",
    "                    # Filter out noise for scoring\n",
    "                    mask = labels != -1\n",
    "                    if mask.sum() > 1:\n",
    "                        score = silhouette_score(subset[mask], labels[mask])\n",
    "                        noise_ratio = (labels == -1).sum() / len(labels)\n",
    "                        \n",
    "                        results.append({\n",
    "                            'eps': eps,\n",
    "                            'min_samples': min_samples,\n",
    "                            'silhouette': score,\n",
    "                            'n_clusters': n_clusters,\n",
    "                            'noise_ratio': noise_ratio\n",
    "                        })\n",
    "                        \n",
    "                        if score > best_score:\n",
    "                            best_score = score\n",
    "                            best_params = {'eps': eps, 'min_samples': min_samples}\n",
    "    \n",
    "    else:  # HDBSCAN\n",
    "        for min_cluster_size in param_grid['min_cluster_size']:\n",
    "            for min_samples in param_grid['min_samples']:\n",
    "                clusterer = hdbscan.HDBSCAN(\n",
    "                    min_cluster_size=min_cluster_size,\n",
    "                    min_samples=min_samples,\n",
    "                    metric='euclidean'\n",
    "                )\n",
    "                labels = clusterer.fit_predict(subset)\n",
    "                \n",
    "                n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "                if n_clusters > 1:\n",
    "                    mask = labels != -1\n",
    "                    if mask.sum() > 1:\n",
    "                        score = silhouette_score(subset[mask], labels[mask])\n",
    "                        noise_ratio = (labels == -1).sum() / len(labels)\n",
    "                        \n",
    "                        results.append({\n",
    "                            'min_cluster_size': min_cluster_size,\n",
    "                            'min_samples': min_samples,\n",
    "                            'silhouette': score,\n",
    "                            'n_clusters': n_clusters,\n",
    "                            'noise_ratio': noise_ratio\n",
    "                        })\n",
    "                        \n",
    "                        if score > best_score:\n",
    "                            best_score = score\n",
    "                            best_params = {\n",
    "                                'min_cluster_size': min_cluster_size,\n",
    "                                'min_samples': min_samples\n",
    "                            }\n",
    "    \n",
    "    if best_params is None:\n",
    "        print(\"⚠ No valid clustering found during optimization. Using default parameters.\")\n",
    "    if config.clustering_method == \"DBSCAN\":\n",
    "        best_params = {'eps': 2.0, 'min_samples': 2}\n",
    "        best_score = 0.0\n",
    "    else:  # HDBSCAN\n",
    "        best_params = {'min_cluster_size': 2, 'min_samples': 1}\n",
    "        best_score = 0.0\n",
    "\n",
    "    print(f\"✓ Best parameters: {best_params}\")\n",
    "    print(f\"✓ Best silhouette score: {best_score:.3f}\")\n",
    "    \n",
    "    return best_params, best_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "196f05d2-1ec4-436b-9162-357ca1d9217c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-05T12:36:44.546392Z",
     "iopub.status.busy": "2025-10-05T12:36:44.546138Z",
     "iopub.status.idle": "2025-10-05T12:36:44.550565Z",
     "shell.execute_reply": "2025-10-05T12:36:44.550003Z",
     "shell.execute_reply.started": "2025-10-05T12:36:44.546371Z"
    }
   },
   "outputs": [],
   "source": [
    "def cluster_features(features: np.ndarray, config: PipelineConfig, params: Dict) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Cluster features using optimized parameters\n",
    "    \n",
    "    Returns:\n",
    "        labels: Cluster assignments for each sample\n",
    "    \"\"\"\n",
    "    print(f\"\\nClustering all {len(features)} samples...\")\n",
    "    \n",
    "    if config.clustering_method == \"DBSCAN\":\n",
    "        clusterer = DBSCAN(**params, metric='euclidean')\n",
    "    else:\n",
    "        clusterer = hdbscan.HDBSCAN(**params, metric='euclidean')\n",
    "    \n",
    "    labels = clusterer.fit_predict(features)\n",
    "    \n",
    "    n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "    n_noise = (labels == -1).sum()\n",
    "    noise_ratio = n_noise / len(labels) * 100\n",
    "    \n",
    "    print(f\"✓ Found {n_clusters} clusters\")\n",
    "    print(f\"✓ Noise samples: {n_noise} ({noise_ratio:.1f}%)\")\n",
    "    \n",
    "    return labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f0cfbcd-6518-4b55-b77b-e37b6b64d688",
   "metadata": {},
   "source": [
    "<h4>JSON Export (COCO Format)</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fcc155c8-c12b-443a-bc1f-4e21e9c338b7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-05T12:36:45.293521Z",
     "iopub.status.busy": "2025-10-05T12:36:45.293261Z",
     "iopub.status.idle": "2025-10-05T12:36:45.300307Z",
     "shell.execute_reply": "2025-10-05T12:36:45.299734Z",
     "shell.execute_reply.started": "2025-10-05T12:36:45.293501Z"
    }
   },
   "outputs": [],
   "source": [
    "def predictions_to_coco_json(\n",
    "    predictions: Dict,\n",
    "    labels: np.ndarray,\n",
    "    image_path: str,\n",
    "    config: PipelineConfig,\n",
    "    clustering_params: Dict,\n",
    "    silhouette: float\n",
    ") -> Dict:\n",
    "    \"\"\"\n",
    "    Convert predictions and cluster labels to COCO format JSON\n",
    "    \"\"\"\n",
    "    print(\"\\nCreating COCO format JSON...\")\n",
    "    \n",
    "    # Generate cluster colors\n",
    "    unique_labels = np.unique(labels)\n",
    "    n_clusters = len(unique_labels) - (1 if -1 in unique_labels else 0)\n",
    "    \n",
    "    # Use a color palette\n",
    "    colors = px.colors.qualitative.Plotly + px.colors.qualitative.Set3\n",
    "    cluster_colors = {}\n",
    "    for i, label in enumerate(unique_labels):\n",
    "        if label == -1:\n",
    "            cluster_colors[-1] = \"#808080\"  # Grey for noise\n",
    "        else:\n",
    "            cluster_colors[label] = colors[i % len(colors)]\n",
    "    \n",
    "    # Count crowns per cluster\n",
    "    cluster_counts = {int(label): int((labels == label).sum()) for label in unique_labels}\n",
    "    \n",
    "    # Build annotations\n",
    "    annotations = []\n",
    "    boxes = predictions['boxes'].numpy()\n",
    "    masks = predictions['masks'].numpy()\n",
    "    scores = predictions['scores'].numpy()\n",
    "    \n",
    "    for i, (box, mask, score, label) in enumerate(zip(boxes, masks, scores, labels)):\n",
    "        x1, y1, x2, y2 = box\n",
    "        w, h = x2 - x1, y2 - y1\n",
    "        \n",
    "        # Convert mask to RLE\n",
    "        mask_fortran = np.asfortranarray(mask.astype(np.uint8))\n",
    "        rle = mask_util.encode(mask_fortran)\n",
    "        rle['counts'] = rle['counts'].decode('utf-8')  # Convert bytes to string\n",
    "        \n",
    "        annotation = {\n",
    "            \"id\": int(i),\n",
    "            \"tile_position\": {\"row\": 0, \"col\": 0},  # Single image, no tiling yet\n",
    "            \"bbox\": [float(x1), float(y1), float(w), float(h)],\n",
    "            \"segmentation\": rle,\n",
    "            \"area\": float(mask.sum()),\n",
    "            \"confidence\": float(score),\n",
    "            \"cluster_id\": int(label)\n",
    "        }\n",
    "        annotations.append(annotation)\n",
    "    \n",
    "    # Build full COCO structure\n",
    "    coco_output = {\n",
    "        \"metadata\": {\n",
    "            \"image_file\": str(image_path),\n",
    "            \"image_dimensions\": [config.original_size, config.original_size],\n",
    "            \"tile_size\": config.original_size,\n",
    "            \"tile_overlap_percent\": 0,  # Single image\n",
    "            \"total_tiles\": 1,\n",
    "            \"clustering_algorithm\": config.clustering_method,\n",
    "            \"clustering_params\": clustering_params,\n",
    "            \"silhouette_score\": float(silhouette)\n",
    "        },\n",
    "        \"annotations\": annotations,\n",
    "        \"clusters\": {\n",
    "            str(label): {\n",
    "                \"count\": cluster_counts[label],\n",
    "                \"color\": cluster_colors[label]\n",
    "            }\n",
    "            for label in unique_labels\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    print(f\"✓ Created {len(annotations)} annotations\")\n",
    "    return coco_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a81b37af-a3ad-4924-8870-bf46a7133dc9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-05T12:36:45.703438Z",
     "iopub.status.busy": "2025-10-05T12:36:45.703196Z",
     "iopub.status.idle": "2025-10-05T12:36:45.706694Z",
     "shell.execute_reply": "2025-10-05T12:36:45.706158Z",
     "shell.execute_reply.started": "2025-10-05T12:36:45.703420Z"
    }
   },
   "outputs": [],
   "source": [
    "def save_json(data: Dict, output_path: str):\n",
    "    \"\"\"Save data to JSON file\"\"\"\n",
    "    with open(output_path, 'w') as f:\n",
    "        json.dump(data, f, indent=2)\n",
    "    print(f\"✓ Saved JSON to {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc0806a3-044e-4c26-a3e0-1d772d2fa4f1",
   "metadata": {},
   "source": [
    "<h4>Visualize Results</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "15518c9b-8de5-4d18-8443-824cb38372e4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-05T12:36:46.288350Z",
     "iopub.status.busy": "2025-10-05T12:36:46.288098Z",
     "iopub.status.idle": "2025-10-05T12:36:46.299270Z",
     "shell.execute_reply": "2025-10-05T12:36:46.298755Z",
     "shell.execute_reply.started": "2025-10-05T12:36:46.288331Z"
    }
   },
   "outputs": [],
   "source": [
    "def visualize_results(\n",
    "    image_np: np.ndarray,\n",
    "    predictions: Dict,\n",
    "    labels: np.ndarray,\n",
    "    cluster_info: Dict,\n",
    "    config: PipelineConfig\n",
    ") -> go.Figure:\n",
    "    \"\"\"\n",
    "    Create interactive Plotly visualization\n",
    "    \"\"\"\n",
    "    print(\"\\nCreating interactive visualization...\")\n",
    "    \n",
    "    # Create subplots\n",
    "    fig = make_subplots(\n",
    "        rows=1, cols=2,\n",
    "        subplot_titles=(\"Clustered Tree Crowns\", \"Cluster Statistics\"),\n",
    "        specs=[[{\"type\": \"image\"}, {\"type\": \"bar\"}]],\n",
    "        column_widths=[0.7, 0.3]\n",
    "    )\n",
    "    \n",
    "    # Prepare overlay image\n",
    "    overlay = image_np.copy()\n",
    "    masks = predictions['masks'].numpy()\n",
    "    \n",
    "    # Get cluster colors from cluster_info\n",
    "    cluster_colors_hex = {int(k): v['color'] for k, v in cluster_info.items()}\n",
    "    \n",
    "    # Overlay masks with cluster colors\n",
    "    for mask, label in zip(masks, labels):\n",
    "        color_hex = cluster_colors_hex[int(label)]\n",
    "        # Convert hex to RGB\n",
    "        color_rgb = tuple(int(color_hex.lstrip('#')[i:i+2], 16) for i in (0, 2, 4))\n",
    "        \n",
    "        # Apply colored mask with transparency\n",
    "        colored_mask = np.zeros_like(overlay)\n",
    "        colored_mask[mask > 0] = color_rgb\n",
    "        overlay = cv2.addWeighted(overlay, 1, colored_mask, 0.5, 0)\n",
    "    \n",
    "    # Add image\n",
    "    fig.add_trace(\n",
    "        go.Image(z=overlay),\n",
    "        row=1, col=1\n",
    "    )\n",
    "    \n",
    "    # Add cluster statistics bar chart\n",
    "    cluster_counts = [v['count'] for k, v in sorted(cluster_info.items()) if int(k) != -1]\n",
    "    cluster_labels_list = [f\"Cluster {k}\" for k in sorted(cluster_info.keys()) if int(k) != -1]\n",
    "    cluster_colors_list = [v['color'] for k, v in sorted(cluster_info.items()) if int(k) != -1]\n",
    "    \n",
    "    # Add noise separately if it exists\n",
    "    if '-1' in cluster_info:\n",
    "        cluster_labels_list.append(\"Noise\")\n",
    "        cluster_counts.append(cluster_info['-1']['count'])\n",
    "        cluster_colors_list.append(cluster_info['-1']['color'])\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Bar(\n",
    "            x=cluster_labels_list,\n",
    "            y=cluster_counts,\n",
    "            marker_color=cluster_colors_list,\n",
    "            text=cluster_counts,\n",
    "            textposition='auto',\n",
    "        ),\n",
    "        row=1, col=2\n",
    "    )\n",
    "    \n",
    "    # Update layout\n",
    "    fig.update_xaxes(title_text=\"Cluster\", row=1, col=2)\n",
    "    fig.update_yaxes(title_text=\"Number of Crowns\", row=1, col=2)\n",
    "    \n",
    "    # Add title with statistics\n",
    "    total_crowns = len(labels)\n",
    "    n_clusters = len([k for k in cluster_info.keys() if int(k) != -1])\n",
    "    noise_pct = (cluster_info.get('-1', {}).get('count', 0) / total_crowns) * 100 if '-1' in cluster_info else 0\n",
    "    \n",
    "    title_text = (\n",
    "        f\"Tree Crown Segmentation Results<br>\"\n",
    "        f\"<sub>Total Crowns: {total_crowns} | Clusters: {n_clusters} | \"\n",
    "        f\"Noise: {noise_pct:.1f}%</sub>\"\n",
    "    )\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title_text=title_text,\n",
    "        height=600,\n",
    "        showlegend=False\n",
    "    )\n",
    "    \n",
    "    print(\"✓ Visualization created\")\n",
    "    return fig\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f2b7c47-dbac-48ec-819a-78114cdfad76",
   "metadata": {},
   "source": [
    "<h4>Main Pipeline</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "22c2c53b-f37a-4987-9790-059b8c3ebd40",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-05T12:36:50.581799Z",
     "iopub.status.busy": "2025-10-05T12:36:50.581546Z",
     "iopub.status.idle": "2025-10-05T12:36:50.588312Z",
     "shell.execute_reply": "2025-10-05T12:36:50.587695Z",
     "shell.execute_reply.started": "2025-10-05T12:36:50.581779Z"
    }
   },
   "outputs": [],
   "source": [
    "def run_pipeline(\n",
    "    config: PipelineConfig,\n",
    "    autoencoder_model,  # Pass your initialized TreeCrownResNet34 here\n",
    "    param_grid: Optional[Dict] = None\n",
    ") -> Tuple[Dict, go.Figure]:\n",
    "    \"\"\"\n",
    "    Run the complete tree crown segmentation and clustering pipeline\n",
    "    \n",
    "    Args:\n",
    "        config: Pipeline configuration\n",
    "        autoencoder_model: Your initialized autoencoder model class\n",
    "        param_grid: Optional custom parameter grid for clustering\n",
    "    \n",
    "    Returns:\n",
    "        coco_json: COCO format dictionary with results\n",
    "        fig: Plotly figure for visualization\n",
    "    \"\"\"\n",
    "    # Create output directory\n",
    "    Path(config.output_dir).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    print(\"=\"*70)\n",
    "    print(\"TREE CROWN SEGMENTATION & CLUSTERING PIPELINE\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # 1. Load models\n",
    "    maskrcnn = load_maskrcnn(config.maskrcnn_checkpoint, config)\n",
    "    autoencoder = load_autoencoder(config.autoencoder_checkpoint, config, autoencoder_model)\n",
    "    \n",
    "    # 2. Load and preprocess image\n",
    "    print(f\"\\nLoading image from {config.image_path}...\")\n",
    "    image_tensor, image_np = load_and_preprocess_image(config.image_path, config.maskrcnn_size)\n",
    "    print(f\"✓ Image loaded: {image_np.shape}\")\n",
    "    \n",
    "    # 3. Run detection\n",
    "    predictions = run_detection(maskrcnn, image_tensor, config)\n",
    "    \n",
    "    if len(predictions['scores']) == 0:\n",
    "        print(\"⚠ No tree crowns detected!\")\n",
    "        return None, None\n",
    "    \n",
    "    # 4. Scale predictions back to original size\n",
    "    predictions_scaled = scale_predictions(\n",
    "        predictions,\n",
    "        from_size=config.maskrcnn_size,\n",
    "        to_size=config.original_size\n",
    "    )\n",
    "    \n",
    "    # Reload original image at full resolution\n",
    "    image_full = np.array(Image.open(config.image_path).convert('RGB'))\n",
    "    \n",
    "    # 5. Crop masks\n",
    "    crops = crop_masks(image_full, predictions_scaled, config)\n",
    "    \n",
    "    # 6. Resize crops for autoencoder\n",
    "    crops_resized = resize_crops(crops, config.autoencoder_size)\n",
    "    \n",
    "    # 7. Extract features\n",
    "    features = extract_features(autoencoder, crops_resized, config)\n",
    "    \n",
    "    # 8. Optimize clustering parameters\n",
    "    best_params, best_score = optimize_clustering_parameters(features, config, param_grid)\n",
    "    \n",
    "    # 9. Cluster all features\n",
    "    labels = cluster_features(features, config, best_params)\n",
    "    \n",
    "    # 10. Export to JSON\n",
    "    coco_json = predictions_to_coco_json(\n",
    "        predictions_scaled,\n",
    "        labels,\n",
    "        config.image_path,\n",
    "        config,\n",
    "        best_params,\n",
    "        best_score\n",
    "    )\n",
    "    \n",
    "    output_json_path = Path(config.output_dir) / \"results.json\"\n",
    "    save_json(coco_json, str(output_json_path))\n",
    "    \n",
    "    # 11. Visualize\n",
    "    fig = visualize_results(\n",
    "        image_full,\n",
    "        predictions_scaled,\n",
    "        labels,\n",
    "        coco_json['clusters'],\n",
    "        config\n",
    "    )\n",
    "    \n",
    "    # Save visualization\n",
    "    output_html_path = Path(config.output_dir) / \"visualization.html\"\n",
    "    fig.write_html(str(output_html_path))\n",
    "    print(f\"✓ Saved interactive visualization to {output_html_path}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"PIPELINE COMPLETE!\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    return coco_json, fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06d8e672-b797-4022-8b76-81a3c97088cd",
   "metadata": {},
   "source": [
    "<h4>Image Loading</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "421f55c4-c34a-4568-8633-8cb1cf868f4c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-06T00:55:28.106679Z",
     "iopub.status.busy": "2025-10-06T00:55:28.106406Z",
     "iopub.status.idle": "2025-10-06T00:55:28.119047Z",
     "shell.execute_reply": "2025-10-06T00:55:28.117984Z",
     "shell.execute_reply.started": "2025-10-06T00:55:28.106657Z"
    }
   },
   "outputs": [],
   "source": [
    "class OrthomosaicReader:\n",
    "    \"\"\"Reads orthomosaics in various formats with tiling support\"\"\"\n",
    "    \n",
    "    def __init__(self, image_path: str):\n",
    "        self.image_path = Path(image_path)\n",
    "        self.format = self._detect_format()\n",
    "        \n",
    "        if self.format == \"geotiff\" and RASTERIO_AVAILABLE:\n",
    "            self.dataset = rasterio.open(str(self.image_path))\n",
    "            self.height = self.dataset.height\n",
    "            self.width = self.dataset.width\n",
    "            self.channels = self.dataset.count\n",
    "        else:\n",
    "            # Use PIL for PNG/JPEG\n",
    "            with Image.open(self.image_path) as img:\n",
    "                self.width, self.height = img.size\n",
    "                self.channels = len(img.getbands())\n",
    "    \n",
    "    def _detect_format(self) -> str:\n",
    "        \"\"\"Detect image format from extension\"\"\"\n",
    "        ext = self.image_path.suffix.lower()\n",
    "        if ext in ['.tif', '.tiff']:\n",
    "            return \"geotiff\"\n",
    "        elif ext in ['.png']:\n",
    "            return \"png\"\n",
    "        elif ext in ['.jpg', '.jpeg']:\n",
    "            return \"jpeg\"\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported format: {ext}\")\n",
    "    \n",
    "    def read_tile(self, row: int, col: int, tile_size: int) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Read a tile from the orthomosaic\n",
    "        \n",
    "        Args:\n",
    "            row: Tile row index\n",
    "            col: Tile column index\n",
    "            tile_size: Size of tile to read\n",
    "            \n",
    "        Returns:\n",
    "            RGB numpy array (H, W, 3)\n",
    "        \"\"\"\n",
    "        x_offset = col * tile_size\n",
    "        y_offset = row * tile_size\n",
    "        \n",
    "        if self.format == \"geotiff\" and RASTERIO_AVAILABLE:\n",
    "            # Read using rasterio (efficient for large files)\n",
    "            window = Window(x_offset, y_offset, tile_size, tile_size)\n",
    "            tile = self.dataset.read(window=window)  # (C, H, W)\n",
    "            tile = np.transpose(tile, (1, 2, 0))  # (H, W, C)\n",
    "            \n",
    "            # Handle different channel counts\n",
    "            if tile.shape[2] == 1:\n",
    "                tile = np.repeat(tile, 3, axis=2)\n",
    "            elif tile.shape[2] == 4:\n",
    "                tile = tile[:, :, :3]  # Drop alpha\n",
    "        else:\n",
    "            # Read using PIL\n",
    "            with Image.open(self.image_path) as img:\n",
    "                box = (x_offset, y_offset, \n",
    "                       min(x_offset + tile_size, self.width),\n",
    "                       min(y_offset + tile_size, self.height))\n",
    "                tile = img.crop(box)\n",
    "                tile = np.array(tile.convert('RGB'))\n",
    "        \n",
    "        return tile\n",
    "    \n",
    "    def get_downsampled_preview(self, max_size: int = 2048) -> np.ndarray:\n",
    "        \"\"\"Get a downsampled version of the full orthomosaic for preview\"\"\"\n",
    "        scale = max(self.width / max_size, self.height / max_size, 1.0)\n",
    "        new_width = int(self.width / scale)\n",
    "        new_height = int(self.height / scale)\n",
    "        \n",
    "        if self.format == \"geotiff\" and RASTERIO_AVAILABLE:\n",
    "            # Read with resampling\n",
    "            data = self.dataset.read(\n",
    "                out_shape=(self.dataset.count, new_height, new_width),\n",
    "                resampling=rasterio.enums.Resampling.bilinear\n",
    "            )\n",
    "            preview = np.transpose(data, (1, 2, 0))\n",
    "            if preview.shape[2] == 1:\n",
    "                preview = np.repeat(preview, 3, axis=2)\n",
    "            elif preview.shape[2] == 4:\n",
    "                preview = preview[:, :, :3]\n",
    "        else:\n",
    "            with Image.open(self.image_path) as img:\n",
    "                img_resized = img.resize((new_width, new_height), Image.BILINEAR)\n",
    "                preview = np.array(img_resized.convert('RGB'))\n",
    "        \n",
    "        return preview\n",
    "    \n",
    "    def close(self):\n",
    "        \"\"\"Close file handles\"\"\"\n",
    "        if hasattr(self, 'dataset') and self.dataset is not None:\n",
    "            self.dataset.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c4fb023-4384-411d-b1f7-382be8997b59",
   "metadata": {},
   "source": [
    "<h4>Tiling System</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "df061fc5-04d0-4538-a682-0652976a4b9e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-06T00:55:58.023094Z",
     "iopub.status.busy": "2025-10-06T00:55:58.022827Z",
     "iopub.status.idle": "2025-10-06T00:55:58.026803Z",
     "shell.execute_reply": "2025-10-06T00:55:58.026186Z",
     "shell.execute_reply.started": "2025-10-06T00:55:58.023074Z"
    }
   },
   "outputs": [],
   "source": [
    "def calculate_tile_grid(image_width: int, image_height: int, \n",
    "                       tile_size: int, overlap_percent: int) -> Tuple[int, int, int]:\n",
    "    \"\"\"\n",
    "    Calculate tile grid dimensions\n",
    "    \n",
    "    Returns:\n",
    "        n_rows, n_cols, overlap_pixels\n",
    "    \"\"\"\n",
    "    overlap_pixels = int(tile_size * overlap_percent / 100)\n",
    "    stride = tile_size - overlap_pixels\n",
    "    \n",
    "    n_cols = int(np.ceil((image_width - overlap_pixels) / stride))\n",
    "    n_rows = int(np.ceil((image_height - overlap_pixels) / stride))\n",
    "    \n",
    "    return n_rows, n_cols, overlap_pixels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b9a62b86-2df3-4f93-9a1b-1cd454e565ab",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-06T00:56:09.237541Z",
     "iopub.status.busy": "2025-10-06T00:56:09.237267Z",
     "iopub.status.idle": "2025-10-06T00:56:09.242134Z",
     "shell.execute_reply": "2025-10-06T00:56:09.241492Z",
     "shell.execute_reply.started": "2025-10-06T00:56:09.237521Z"
    }
   },
   "outputs": [],
   "source": [
    "def generate_tile_coordinates(image_width: int, image_height: int,\n",
    "                              tile_size: int, overlap_pixels: int) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Generate coordinates for all tiles\n",
    "    \n",
    "    Returns:\n",
    "        List of tile metadata dicts\n",
    "    \"\"\"\n",
    "    stride = tile_size - overlap_pixels\n",
    "    tiles = []\n",
    "    \n",
    "    row = 0\n",
    "    y = 0\n",
    "    while y < image_height:\n",
    "        col = 0\n",
    "        x = 0\n",
    "        while x < image_width:\n",
    "            # Calculate actual tile bounds\n",
    "            x_end = min(x + tile_size, image_width)\n",
    "            y_end = min(y + tile_size, image_height)\n",
    "            actual_width = x_end - x\n",
    "            actual_height = y_end - y\n",
    "            \n",
    "            tiles.append({\n",
    "                'row': row,\n",
    "                'col': col,\n",
    "                'x': x,\n",
    "                'y': y,\n",
    "                'width': actual_width,\n",
    "                'height': actual_height,\n",
    "                'tile_id': f\"tile_{row}_{col}\"\n",
    "            })\n",
    "            \n",
    "            x += stride\n",
    "            col += 1\n",
    "        \n",
    "        y += stride\n",
    "        row += 1\n",
    "    \n",
    "    return tiles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3186a679-4dac-4069-8dcc-90778af609a6",
   "metadata": {},
   "source": [
    "<h4>Duplicate Detection Across Tiles</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "02002e41-fbe5-4a70-bde4-b0f1762cbc2e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-06T00:56:46.639944Z",
     "iopub.status.busy": "2025-10-06T00:56:46.639682Z",
     "iopub.status.idle": "2025-10-06T00:56:46.644231Z",
     "shell.execute_reply": "2025-10-06T00:56:46.643490Z",
     "shell.execute_reply.started": "2025-10-06T00:56:46.639926Z"
    }
   },
   "outputs": [],
   "source": [
    "def compute_iou(box1: np.ndarray, box2: np.ndarray) -> float:\n",
    "    \"\"\"Compute IoU between two bounding boxes [x1, y1, x2, y2]\"\"\"\n",
    "    x1 = max(box1[0], box2[0])\n",
    "    y1 = max(box1[1], box2[1])\n",
    "    x2 = min(box1[2], box2[2])\n",
    "    y2 = min(box1[3], box2[3])\n",
    "    \n",
    "    if x2 < x1 or y2 < y1:\n",
    "        return 0.0\n",
    "    \n",
    "    intersection = (x2 - x1) * (y2 - y1)\n",
    "    area1 = (box1[2] - box1[0]) * (box1[3] - box1[1])\n",
    "    area2 = (box2[2] - box2[0]) * (box2[3] - box2[1])\n",
    "    union = area1 + area2 - intersection\n",
    "    \n",
    "    return intersection / union if union > 0 else 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "420c5ec4-e0c7-4da2-9e1b-dc70489aa3d7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-06T00:56:58.791186Z",
     "iopub.status.busy": "2025-10-06T00:56:58.790911Z",
     "iopub.status.idle": "2025-10-06T00:56:58.797824Z",
     "shell.execute_reply": "2025-10-06T00:56:58.797291Z",
     "shell.execute_reply.started": "2025-10-06T00:56:58.791153Z"
    }
   },
   "outputs": [],
   "source": [
    "def merge_duplicate_detections(all_detections: List[Dict], \n",
    "                               iou_threshold: float = 0.5) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Merge duplicate detections across tiles based on IoU\n",
    "    \n",
    "    Args:\n",
    "        all_detections: List of detection dicts with global coordinates\n",
    "        iou_threshold: IoU threshold for considering duplicates\n",
    "        \n",
    "    Returns:\n",
    "        List of unique detections\n",
    "    \"\"\"\n",
    "    if len(all_detections) == 0:\n",
    "        return []\n",
    "    \n",
    "    print(f\"\\nMerging duplicate detections (IoU threshold: {iou_threshold})...\")\n",
    "    print(f\"Total detections before merging: {len(all_detections)}\")\n",
    "    \n",
    "    # Sort by confidence (descending)\n",
    "    detections = sorted(all_detections, key=lambda x: x['confidence'], reverse=True)\n",
    "    \n",
    "    # Track which detections to keep\n",
    "    keep = [True] * len(detections)\n",
    "    \n",
    "    for i in range(len(detections)):\n",
    "        if not keep[i]:\n",
    "            continue\n",
    "            \n",
    "        box1 = np.array([\n",
    "            detections[i]['global_bbox'][0],\n",
    "            detections[i]['global_bbox'][1],\n",
    "            detections[i]['global_bbox'][0] + detections[i]['global_bbox'][2],\n",
    "            detections[i]['global_bbox'][1] + detections[i]['global_bbox'][3]\n",
    "        ])\n",
    "        \n",
    "        for j in range(i + 1, len(detections)):\n",
    "            if not keep[j]:\n",
    "                continue\n",
    "                \n",
    "            box2 = np.array([\n",
    "                detections[j]['global_bbox'][0],\n",
    "                detections[j]['global_bbox'][1],\n",
    "                detections[j]['global_bbox'][0] + detections[j]['global_bbox'][2],\n",
    "                detections[j]['global_bbox'][1] + detections[j]['global_bbox'][3]\n",
    "            ])\n",
    "            \n",
    "            iou = compute_iou(box1, box2)\n",
    "            \n",
    "            if iou >= iou_threshold:\n",
    "                # Mark lower confidence detection for removal\n",
    "                keep[j] = False\n",
    "    \n",
    "    unique_detections = [det for det, k in zip(detections, keep) if k]\n",
    "    print(f\"✓ Unique detections after merging: {len(unique_detections)}\")\n",
    "    print(f\"  Removed {len(detections) - len(unique_detections)} duplicates\")\n",
    "    \n",
    "    return unique_detections"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "368ae2a3-8cb8-4df8-a95f-b5b2c40c5358",
   "metadata": {},
   "source": [
    "<h4>Main Orthomosaic Pipeline</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "da41b64e-652f-4ced-b68e-33f968ea25b5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-06T00:57:33.228585Z",
     "iopub.status.busy": "2025-10-06T00:57:33.228322Z",
     "iopub.status.idle": "2025-10-06T00:57:33.238705Z",
     "shell.execute_reply": "2025-10-06T00:57:33.237927Z",
     "shell.execute_reply.started": "2025-10-06T00:57:33.228564Z"
    }
   },
   "outputs": [],
   "source": [
    "def process_single_tile(\n",
    "    tile_info: Dict,\n",
    "    reader: OrthomosaicReader,\n",
    "    maskrcnn: pl.LightningModule,\n",
    "    autoencoder,\n",
    "    config: OrthomosaicConfig\n",
    ") -> Tuple[List[Dict], Optional[np.ndarray]]:\n",
    "    \"\"\"\n",
    "    Process a single tile through detection and feature extraction\n",
    "    \n",
    "    Returns:\n",
    "        detections: List of detection dicts with features\n",
    "        preview_image: Optional preview of tile with detections\n",
    "    \"\"\"\n",
    "    # Read tile\n",
    "    tile_rgb = reader.read_tile(\n",
    "        tile_info['row'], \n",
    "        tile_info['col'], \n",
    "        config.tile_size\n",
    "    )\n",
    "    \n",
    "    # Resize for Mask R-CNN\n",
    "    tile_resized = cv2.resize(tile_rgb, (config.maskrcnn_size, config.maskrcnn_size))\n",
    "    tile_tensor = torchvision.transforms.functional.to_tensor(tile_resized)\n",
    "    \n",
    "    # Run detection\n",
    "    with torch.no_grad():\n",
    "        image_batch = tile_tensor.unsqueeze(0).to(config.device)\n",
    "        predictions = maskrcnn(image_batch)\n",
    "        predictions_processed = maskrcnn.pred_processor(predictions)\n",
    "    \n",
    "    # Move to CPU\n",
    "    for key in predictions_processed:\n",
    "        predictions_processed[key] = predictions_processed[key].cpu()\n",
    "    \n",
    "    if len(predictions_processed['scores']) == 0:\n",
    "        return [], None\n",
    "    \n",
    "    # Scale predictions back to tile size\n",
    "    scale_factor = config.tile_size / config.maskrcnn_size\n",
    "    boxes = predictions_processed['boxes'].numpy() * scale_factor\n",
    "    masks = []\n",
    "    for mask in predictions_processed['masks'].numpy():\n",
    "        scaled_mask = cv2.resize(\n",
    "            mask.astype(np.uint8),\n",
    "            (config.tile_size, config.tile_size),\n",
    "            interpolation=cv2.INTER_NEAREST\n",
    "        )\n",
    "        masks.append(scaled_mask)\n",
    "    \n",
    "    # Process each detection\n",
    "    detections = []\n",
    "    for i, (box, mask) in enumerate(zip(boxes, masks)):\n",
    "        x1, y1, x2, y2 = box.astype(int)\n",
    "        x1, y1 = max(0, x1), max(0, y1)\n",
    "        x2 = min(tile_rgb.shape[1], x2)\n",
    "        y2 = min(tile_rgb.shape[0], y2)\n",
    "        \n",
    "        # Crop and resize for autoencoder\n",
    "        crop_rgb = tile_rgb[y1:y2, x1:x2]\n",
    "        crop_mask = mask[y1:y2, x1:x2]\n",
    "        \n",
    "        # Create RGBA\n",
    "        crop_rgba = np.zeros((*crop_rgb.shape[:2], 4), dtype=np.uint8)\n",
    "        crop_rgba[:, :, :3] = crop_rgb\n",
    "        crop_rgba[:, :, 3] = (crop_mask * 255).astype(np.uint8)\n",
    "        \n",
    "        # Resize for autoencoder\n",
    "        crop_resized = cv2.resize(crop_rgba, (config.autoencoder_size, config.autoencoder_size))\n",
    "        \n",
    "        # Extract features\n",
    "        rgb = crop_resized[:, :, :3].astype(np.float32) / 255.0\n",
    "        tensor = torch.from_numpy(rgb).permute(2, 0, 1).unsqueeze(0).to(config.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            features = autoencoder.encode(tensor).cpu().numpy().squeeze()\n",
    "        \n",
    "        # Convert mask to RLE\n",
    "        mask_fortran = np.asfortranarray(mask.astype(np.uint8))\n",
    "        rle = mask_util.encode(mask_fortran)\n",
    "        rle['counts'] = rle['counts'].decode('utf-8')\n",
    "        \n",
    "        # Store detection with global coordinates\n",
    "        detection = {\n",
    "            'tile_id': tile_info['tile_id'],\n",
    "            'tile_row': tile_info['row'],\n",
    "            'tile_col': tile_info['col'],\n",
    "            'local_bbox': [float(x1), float(y1), float(x2-x1), float(y2-y1)],\n",
    "            'global_bbox': [\n",
    "                float(tile_info['x'] + x1),\n",
    "                float(tile_info['y'] + y1),\n",
    "                float(x2 - x1),\n",
    "                float(y2 - y1)\n",
    "            ],\n",
    "            'segmentation': rle,\n",
    "            'area': float(mask.sum()),\n",
    "            'confidence': float(predictions_processed['scores'][i]),\n",
    "            'features': features\n",
    "        }\n",
    "        detections.append(detection)\n",
    "    \n",
    "    # Create preview if enabled\n",
    "    preview_image = None\n",
    "    if config.enable_tile_previews:\n",
    "        preview_image = tile_rgb.copy()\n",
    "        for mask in masks:\n",
    "            preview_image[mask > 0] = preview_image[mask > 0] * 0.5 + np.array([0, 255, 0]) * 0.5\n",
    "    \n",
    "    return detections, preview_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2f05659a-23b8-4d00-a6da-3118126a4923",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-06T00:58:02.254858Z",
     "iopub.status.busy": "2025-10-06T00:58:02.254588Z",
     "iopub.status.idle": "2025-10-06T00:58:02.266699Z",
     "shell.execute_reply": "2025-10-06T00:58:02.266152Z",
     "shell.execute_reply.started": "2025-10-06T00:58:02.254839Z"
    }
   },
   "outputs": [],
   "source": [
    "def run_orthomosaic_pipeline(\n",
    "    config: OrthomosaicConfig,\n",
    "    autoencoder_model,\n",
    "    param_grid: Optional[Dict] = None\n",
    ") -> Tuple[Dict, go.Figure]:\n",
    "    \"\"\"\n",
    "    Run complete orthomosaic processing pipeline\n",
    "    \"\"\"\n",
    "    output_dir = Path(config.output_dir)\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(\"ORTHOMOSAIC TREE CROWN SEGMENTATION PIPELINE\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Load models (reuse from single image pipeline)\n",
    "    from tree_crown_pipeline import load_maskrcnn, load_autoencoder\n",
    "    print(\"\\n1. Loading models...\")\n",
    "    maskrcnn = load_maskrcnn(config.maskrcnn_checkpoint, config)\n",
    "    autoencoder = load_autoencoder(config.autoencoder_checkpoint, config, autoencoder_model)\n",
    "    \n",
    "    # Open orthomosaic\n",
    "    print(f\"\\n2. Opening orthomosaic: {config.orthomosaic_path}\")\n",
    "    reader = OrthomosaicReader(config.orthomosaic_path)\n",
    "    print(f\"   Dimensions: {reader.width} × {reader.height}\")\n",
    "    print(f\"   Format: {reader.format}\")\n",
    "    \n",
    "    # Calculate tiling\n",
    "    print(f\"\\n3. Calculating tile grid...\")\n",
    "    n_rows, n_cols, overlap_pixels = calculate_tile_grid(\n",
    "        reader.width, reader.height,\n",
    "        config.tile_size, config.tile_overlap_percent\n",
    "    )\n",
    "    tiles = generate_tile_coordinates(\n",
    "        reader.width, reader.height,\n",
    "        config.tile_size, overlap_pixels\n",
    "    )\n",
    "    print(f\"   Grid: {n_rows} rows × {n_cols} cols = {len(tiles)} tiles\")\n",
    "    print(f\"   Overlap: {overlap_pixels} pixels ({config.tile_overlap_percent}%)\")\n",
    "    \n",
    "    # Process tiles\n",
    "    print(f\"\\n4. Processing tiles...\")\n",
    "    all_detections = []\n",
    "    tile_previews = []\n",
    "    \n",
    "    for tile_info in tqdm(tiles, desc=\"Processing tiles\"):\n",
    "        detections, preview = process_single_tile(\n",
    "            tile_info, reader, maskrcnn, autoencoder, config\n",
    "        )\n",
    "        \n",
    "        all_detections.extend(detections)\n",
    "        \n",
    "        if preview is not None:\n",
    "            tile_previews.append({\n",
    "                'tile_id': tile_info['tile_id'],\n",
    "                'image': preview,\n",
    "                'n_detections': len(detections)\n",
    "            })\n",
    "        \n",
    "        # Save intermediate results\n",
    "        if config.save_intermediate and len(detections) > 0:\n",
    "            tile_output = output_dir / f\"{tile_info['tile_id']}.json\"\n",
    "            with open(tile_output, 'w') as f:\n",
    "                json.dump({\n",
    "                    'tile_info': tile_info,\n",
    "                    'detections': [{k: v.tolist() if isinstance(v, np.ndarray) else v \n",
    "                                   for k, v in det.items() if k != 'features'} \n",
    "                                  for det in detections]\n",
    "                }, f)\n",
    "    \n",
    "    print(f\"✓ Processed {len(tiles)} tiles\")\n",
    "    print(f\"✓ Total detections: {len(all_detections)}\")\n",
    "    \n",
    "    # Merge duplicates\n",
    "    print(f\"\\n5. Merging duplicate detections...\")\n",
    "    unique_detections = merge_duplicate_detections(all_detections, config.iou_threshold)\n",
    "    \n",
    "    # Extract features for clustering\n",
    "    print(f\"\\n6. Preparing features for clustering...\")\n",
    "    features = np.array([det['features'] for det in unique_detections])\n",
    "    print(f\"   Feature matrix shape: {features.shape}\")\n",
    "    \n",
    "    # Cluster (reuse from single image pipeline)\n",
    "    from tree_crown_pipeline import optimize_clustering_parameters, cluster_features\n",
    "    \n",
    "    print(f\"\\n7. Clustering...\")\n",
    "    best_params, best_score = optimize_clustering_parameters(features, config, param_grid)\n",
    "    labels = cluster_features(features, config, best_params)\n",
    "    \n",
    "    # Assign cluster labels to detections\n",
    "    for det, label in zip(unique_detections, labels):\n",
    "        det['cluster_id'] = int(label)\n",
    "        del det['features']  # Remove features to save space\n",
    "    \n",
    "    # Create output JSON\n",
    "    print(f\"\\n8. Creating output JSON...\")\n",
    "    \n",
    "    unique_labels = np.unique(labels)\n",
    "    colors = px.colors.qualitative.Plotly + px.colors.qualitative.Set3\n",
    "    cluster_info = {}\n",
    "    for i, label in enumerate(unique_labels):\n",
    "        cluster_info[str(int(label))] = {\n",
    "            'count': int((labels == label).sum()),\n",
    "            'color': \"#808080\" if label == -1 else colors[i % len(colors)]\n",
    "        }\n",
    "    \n",
    "    output_json = {\n",
    "        'metadata': {\n",
    "            'orthomosaic_file': str(config.orthomosaic_path),\n",
    "            'orthomosaic_dimensions': [reader.height, reader.width],\n",
    "            'tile_size': config.tile_size,\n",
    "            'tile_overlap_percent': config.tile_overlap_percent,\n",
    "            'total_tiles': len(tiles),\n",
    "            'tiles_processed': len(tiles),\n",
    "            'clustering_algorithm': config.clustering_method,\n",
    "            'clustering_params': best_params,\n",
    "            'silhouette_score': float(best_score)\n",
    "        },\n",
    "        'annotations': unique_detections,\n",
    "        'clusters': cluster_info\n",
    "    }\n",
    "    \n",
    "    output_json_path = output_dir / \"orthomosaic_results.json\"\n",
    "    with open(output_json_path, 'w') as f:\n",
    "        json.dump(output_json, f, indent=2)\n",
    "    print(f\"✓ Saved results to {output_json_path}\")\n",
    "    \n",
    "    # Visualization\n",
    "    print(f\"\\n9. Creating visualizations...\")\n",
    "    \n",
    "    # Get downsampled preview\n",
    "    preview = reader.get_downsampled_preview(config.preview_max_size)\n",
    "    \n",
    "    # Create visualization\n",
    "    fig = create_orthomosaic_visualization(\n",
    "        preview, unique_detections, cluster_info,\n",
    "        reader.width, reader.height, config\n",
    "    )\n",
    "    \n",
    "    output_html_path = output_dir / \"orthomosaic_visualization.html\"\n",
    "    fig.write_html(str(output_html_path))\n",
    "    print(f\"✓ Saved visualization to {output_html_path}\")\n",
    "    \n",
    "    # Cleanup\n",
    "    reader.close()\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"PIPELINE COMPLETE!\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Total detections: {len(unique_detections)}\")\n",
    "    print(f\"Clusters found: {len([k for k in cluster_info.keys() if int(k) != -1])}\")\n",
    "    \n",
    "    return output_json, fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "268bd6b1-2fd5-4e53-ab9f-ca8300a1d76d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-06T00:58:29.054521Z",
     "iopub.status.busy": "2025-10-06T00:58:29.054251Z",
     "iopub.status.idle": "2025-10-06T00:58:29.061981Z",
     "shell.execute_reply": "2025-10-06T00:58:29.061423Z",
     "shell.execute_reply.started": "2025-10-06T00:58:29.054500Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_orthomosaic_visualization(\n",
    "    preview_image: np.ndarray,\n",
    "    detections: List[Dict],\n",
    "    cluster_info: Dict,\n",
    "    original_width: int,\n",
    "    original_height: int,\n",
    "    config: OrthomosaicConfig\n",
    ") -> go.Figure:\n",
    "    \"\"\"Create interactive visualization of orthomosaic results\"\"\"\n",
    "    \n",
    "    # Calculate scale factor\n",
    "    scale_y = preview_image.shape[0] / original_height\n",
    "    scale_x = preview_image.shape[1] / original_width\n",
    "    \n",
    "    # Create overlay\n",
    "    overlay = preview_image.copy()\n",
    "    \n",
    "    for det in detections:\n",
    "        cluster_id = det['cluster_id']\n",
    "        color_hex = cluster_info[str(cluster_id)]['color']\n",
    "        color_rgb = tuple(int(color_hex.lstrip('#')[i:i+2], 16) for i in (0, 2, 4))\n",
    "        \n",
    "        # Scale bbox to preview size\n",
    "        x = int(det['global_bbox'][0] * scale_x)\n",
    "        y = int(det['global_bbox'][1] * scale_y)\n",
    "        w = int(det['global_bbox'][2] * scale_x)\n",
    "        h = int(det['global_bbox'][3] * scale_y)\n",
    "        \n",
    "        # Draw bbox\n",
    "        cv2.rectangle(overlay, (x, y), (x+w, y+h), color_rgb, 2)\n",
    "    \n",
    "    # Create figure\n",
    "    fig = make_subplots(\n",
    "        rows=1, cols=2,\n",
    "        subplot_titles=(\"Orthomosaic Overview\", \"Cluster Statistics\"),\n",
    "        specs=[[{\"type\": \"image\"}, {\"type\": \"bar\"}]],\n",
    "        column_widths=[0.7, 0.3]\n",
    "    )\n",
    "    \n",
    "    fig.add_trace(go.Image(z=overlay), row=1, col=1)\n",
    "    \n",
    "    # Cluster stats\n",
    "    cluster_labels = [f\"Cluster {k}\" for k in sorted(cluster_info.keys()) if int(k) != -1]\n",
    "    cluster_counts = [v['count'] for k, v in sorted(cluster_info.items()) if int(k) != -1]\n",
    "    cluster_colors = [v['color'] for k, v in sorted(cluster_info.items()) if int(k) != -1]\n",
    "    \n",
    "    if '-1' in cluster_info:\n",
    "        cluster_labels.append(\"Noise\")\n",
    "        cluster_counts.append(cluster_info['-1']['count'])\n",
    "        cluster_colors.append(cluster_info['-1']['color'])\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Bar(x=cluster_labels, y=cluster_counts, marker_color=cluster_colors),\n",
    "        row=1, col=2\n",
    "    )\n",
    "    \n",
    "    # Title\n",
    "    total_crowns = len(detections)\n",
    "    n_clusters = len([k for k in cluster_info.keys() if int(k) != -1])\n",
    "    noise_pct = (cluster_info.get('-1', {}).get('count', 0) / total_crowns) * 100 if total_crowns > 0 else 0\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title_text=f\"Orthomosaic Analysis<br><sub>Total Crowns: {total_crowns} | Clusters: {n_clusters} | Noise: {noise_pct:.1f}%</sub>\",\n",
    "        height=800,\n",
    "        showlegend=False\n",
    "    )\n",
    "    \n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57137183-0c02-4df9-ac06-0b45871db202",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fb4f8c60-3b50-4a42-b29c-1c625e355564",
   "metadata": {},
   "source": [
    "<h2>Execution Pipeline</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b4ace06-9d5f-480c-9499-d34d0bc00116",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
