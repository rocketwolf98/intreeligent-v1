{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa424e3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kennywan/Documents/intreeligent-v1/.venv/lib64/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#import packages\n",
    "import modal\n",
    "import os\n",
    "import xml.etree.ElementTree as ET\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import segmentation_models_pytorch as smp\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ae49524",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up Modal App for intreeligent\n",
    "app = modal.App(\"intreeligent-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a7d59c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up Modal image\n",
    "image = (\n",
    "    modal.Image.debian_slim(python_version=\"3.13.7\")\n",
    "    .pip_install_from_requirements(\"requirements.txt\")\n",
    "    .apt_install(\"libgl1-mesa-glx\" \"libglib2.0-0\")  # for cv2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "483b3312",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create volume for data\n",
    "volume = modal.Volume.from_name(\"treesdataset\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6c5a7d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting XML annotations to masks\n",
    "def xml_to_mask(xml_path, img_shape):\n",
    "    tree = ET.parse(xml_path)\n",
    "    root = tree.getroot()\n",
    "    mask = np.zeros(img_shape[:2], dtype=np.uint8)\n",
    "\n",
    "    for obj in root.findall('.//object'):\n",
    "        tree_element = obj.find('tree')\n",
    "        if tree_element is not None:\n",
    "            bbox = obj.find('bndbox')\n",
    "            if bbox is not None:\n",
    "                xmin = int(bbox.find('xmin').text)\n",
    "                ymin = int(bbox.find('ymin').text)\n",
    "                xmax = int(bbox.find('xmax').text)\n",
    "                ymax = int(bbox.find('ymax').text)\n",
    "                \n",
    "                center_x = (xmin + xmax) // 2\n",
    "                center_y = (ymin + ymax) // 2\n",
    "                width = (xmax - xmin) // 2\n",
    "                height = (ymax - ymin) // 2\n",
    "\n",
    "                cv2.ellipse(mask, (center_x, center_y), (width, height), 0, 0, 360, 1, -1)\n",
    "\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5b2b287d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TreeDataset(Dataset):\n",
    "    def __init__(self, image_dir, xml_dir, transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.xml_dir = xml_dir\n",
    "        self.transform = transform\n",
    "        self.image_files = [f for f in os.listdir(image_dir) if f.endswith(('.tif', '.tiff', '.jpg', '.png'))]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.image_files[idx]\n",
    "        img_path = os.path.join(self.image_dir, img_name)\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "\n",
    "        xml_name = img_name.replace('.tif', '.xml').replace('.tiff', '.xml').replace('.jpg', '.xml').replace('.png', '.xml')\n",
    "        xml_path = os.path.join(self.xml_dir, xml_name)\n",
    "\n",
    "        image_array = np.array(image)\n",
    "        mask = xml_to_mask(xml_path, image_array.shape)\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        mask = torch.from_numpy(mask).long()\n",
    "        return image, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dc9c41b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_tree_features(image, mask):\n",
    "    \"\"\"Extract features for each tree in the mask.\"\"\"\n",
    "    features_list = []\n",
    "    contours, _ = cv2.findContours(mask.numpy().astype(np.uint8), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    for contour in contours:\n",
    "        if cv2.contourArea(contour) < 50:\n",
    "            continue\n",
    "\n",
    "        x, y, w, h = cv2.boundingRect(contour)\n",
    "        tree_region = image[y:y+h, x:x+w]\n",
    "        tree_mask = np.zeros((h, w), dtype=np.uint8)\n",
    "        cv2.filPoly(tree_mask, contour - [x, y], 1)\n",
    "\n",
    "        features = []\n",
    "\n",
    "        for channel in range(3):\n",
    "            channel_pixels = tree_region[:, :, channel][tree_mask > 0]\n",
    "            if len(channel_pixels) > 0:\n",
    "                features.append(np.mean(channel_pixels))\n",
    "                features.append(np.std(channel_pixels))\n",
    "            else:\n",
    "                features.extend([0,0])\n",
    "\n",
    "        area = cv2.contourArea(contour)\n",
    "        features.append(area)\n",
    "        features.append(w/h)\n",
    "\n",
    "        features_list.append(features)\n",
    "\n",
    "    return np.array(features_list)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "da4bae11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will execute with Modal functions\n",
    "@app.function(\n",
    "    image=image,\n",
    "    gpu=\"A100\",\n",
    "    memory=32768,\n",
    "    timeout=3600,\n",
    "    volumes={\"/data\": volume}\n",
    ")\n",
    "\n",
    "def upload_data():\n",
    "    print(\"Please upload your dataset to /data/ using Modal CLI\")\n",
    "    print(\" Modal volume put treesdataset local/path/to/train /data/train\")\n",
    "    print(\" Modal volume put treesdataset local/path/to/annotations /data/annotations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "25261158",
   "metadata": {},
   "outputs": [],
   "source": [
    "@app.function(\n",
    "    image=image,\n",
    "    gpu=\"A100\",\n",
    "    memory=32768,\n",
    "    timeout=7200,\n",
    "    volumes={\"/data\": volume}\n",
    ")\n",
    "\n",
    "def train_model():\n",
    "    print(\"Starting model training...\")\n",
    "\n",
    "    IMAGE_DIR = \"/data/train\"\n",
    "    XML_DIR = \"/data/annotations\"\n",
    "\n",
    "    if not os.path.exists(IMAGE_DIR) or not os.path.exists(XML_DIR):\n",
    "        raise FileNotFoundError(\"Image or XML directory not found in /data/. Please upload your dataset.\")\n",
    "    \n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((128, 128)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "\n",
    "    dataset = TreeDataset(IMAGE_DIR, XML_DIR, transform=transform)\n",
    "    dataloader = DataLoader(dataset, batch_size=8, shuffle=True, num_workers=4)\n",
    "\n",
    "    print(f\"Dataset: {len(dataset)} samples, {len(dataloader)} batches\")\n",
    "\n",
    "    model = smp.Unet(encoder_name=\"resnet34\", \n",
    "                     encoder_weights=\"imagenet\", \n",
    "                     in_channels=3, \n",
    "                     classes=2).cuda()\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(3):\n",
    "        total_loss = 0\n",
    "        progress_bar = tqdm(dataloader, desc=f\"Epoch {epoch+1}/3\")\n",
    "\n",
    "        for batch_idx, (images, masks) in enumerate(progress_bar):\n",
    "            images, masks = images.cuda(), masks.cuda()\n",
    "\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, masks)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            progress_bar.set_postfix({\n",
    "                'Loss' : f'{loss.item():.4f}',\n",
    "                'Avg Loss' : f'{(total_loss/(batch_idx+1)):.4f}'\n",
    "            })\n",
    "\n",
    "            torch.cuda.empty_cache()\n",
    "        \n",
    "        print(f\"Epoch {epoch+1} completed. Average Loss: {total_loss/len(dataloader):.4f}\")\n",
    "\n",
    "    torch.save(model.state_dict(), \"/data/tree_segmentation_model.pth\")\n",
    "    print(\"Model training completed and saved to /data/tree_segmentation_model.pth\")\n",
    "\n",
    "    return \"Training completed successfully.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "37f0bcaa",
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidError",
     "evalue": "Duplicate local entrypoint name: main. Local entrypoint names must be unique.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mInvalidError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Test Execution\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;129;43m@app\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlocal_entrypoint\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;43;01mdef\u001b[39;49;00m\u001b[38;5;250;43m \u001b[39;49m\u001b[34;43mmain\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[38;5;250;43m    \u001b[39;49m\u001b[33;43;03m\"\"\"Main execution pipeline\"\"\"\u001b[39;49;00m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mprint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m🌲 Modal Tree Crown Segmentation Pipeline\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/intreeligent-v1/.venv/lib64/python3.13/site-packages/synchronicity/synchronizer.py:574\u001b[39m, in \u001b[36mSynchronizer._wrap_callable.<locals>.f_wrapped.<locals>.f_wrapped\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    572\u001b[39m args = \u001b[38;5;28mself\u001b[39m._translate_in(args)\n\u001b[32m    573\u001b[39m kwargs = \u001b[38;5;28mself\u001b[39m._translate_in(kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m574\u001b[39m f_res = \u001b[43mres\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    575\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(f, \u001b[38;5;28mself\u001b[39m._output_translation_attr, \u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[32m    576\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._translate_out(f_res)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/intreeligent-v1/.venv/lib64/python3.13/site-packages/modal/app.py:606\u001b[39m, in \u001b[36m_App.local_entrypoint.<locals>.wrapped\u001b[39m\u001b[34m(raw_f)\u001b[39m\n\u001b[32m    603\u001b[39m tag = name \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m raw_f.\u001b[34m__qualname__\u001b[39m\n\u001b[32m    604\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m tag \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._local_entrypoints:\n\u001b[32m    605\u001b[39m     \u001b[38;5;66;03m# TODO: get rid of this limitation.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m606\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m InvalidError(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mDuplicate local entrypoint name: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtag\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. Local entrypoint names must be unique.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    607\u001b[39m entrypoint = \u001b[38;5;28mself\u001b[39m._local_entrypoints[tag] = _LocalEntrypoint(info, \u001b[38;5;28mself\u001b[39m)\n\u001b[32m    608\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m entrypoint\n",
      "\u001b[31mInvalidError\u001b[39m: Duplicate local entrypoint name: main. Local entrypoint names must be unique."
     ]
    }
   ],
   "source": [
    "# Test Execution\n",
    "@app.local_entrypoint()\n",
    "def main():\n",
    "    \"\"\"Main execution pipeline\"\"\"\n",
    "    print(\"🌲 Modal Tree Crown Segmentation Pipeline\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Step 1: Upload data (run this first manually)\n",
    "    print(\"1. Upload your data using Modal CLI:\")\n",
    "    print(\"   modal volume put tree-data-volume ./train /data/train\")  \n",
    "    print(\"   modal volume put tree-data-volume ./annotations /data/annotations\")\n",
    "    print()\n",
    "    \n",
    "    # Step 2: Train model\n",
    "    print(\"2. Training model...\")\n",
    "    train_result = train_model.remote()\n",
    "    print(f\"Training result: {train_result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85db5373",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
